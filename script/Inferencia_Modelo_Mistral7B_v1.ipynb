{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferencia Modelo Mistral Google Colab\n",
    "\n",
    "**Autor:** Guillermo Gallego Reina\n",
    "\n",
    "**Fecha:** 14 de Mayo de 2024\n",
    "\n",
    "**Introducción**\n",
    "\n",
    "---\n",
    "\n",
    "En el anterior notebook se explicarón las diferentes etapas para llevar acabo el ajuste de modelo de Mistral dando como resultado un modelo ajustado llamado [mistralAI_recetascocina](https://huggingface.co/guillergalre/mistralAI_recetascocina). Como vimos, este modelo esta ajustado para una dataset de recetas colombinas.\n",
    "\n",
    "Se explicarán las diferentes etapas para llevar a cabo el proceso de validación entre los diferentes modelos. Estas consistirán en realizar un petición al modelo ajustado y no ajustado comprobando la respuesta y tiempo.\n",
    "**#LoRA**, **#GenAI**, **#LLM**, **#Mistral**, **#GPU**, **#QLoRA**, **#GoogleColab**.\n",
    "\n",
    "---\n",
    "\n",
    "**Tareas:**\n",
    "\n",
    "1. **[Importar Librerias](#1)**\n",
    "   - Librerias necesarias para Google Colab.\n",
    "\n",
    "2. **[Preparación Entorno Hugging Face](#2)**\n",
    "   - Realizamos la configuracion necesaria para poder acceder a Hugging Face.\n",
    "\n",
    "3. **[Carga del Modelo](#3)**\n",
    "   - 3.1 [Cargamos el modelo ajustado: mistralAI_recetascocina](#4)\n",
    "   - 3.2 [Cargamos el modelo base: Mistral-7B-Instruct-v0.1](#5)\n",
    "    \n",
    "4. **[Inferencia](#6)**\n",
    "    - 4.1 [Preprocesado de acuerdo a la plantilla](#7)\n",
    "    - 4.2 [Inferencia Modelo Ajustado](#8)\n",
    "    - 4.3 [Inferencia Modelo Base](#9)\n",
    "\n",
    "4. **[Conclusiones](#10)**\n",
    "   - Se obtienen las siguientes conclusiones.\n",
    "\n",
    "\n",
    "**Entorno de Trabajo:**\n",
    "   - Se utilizará **Google Colab** por su fácil acceso a GPU y TPU de alto rendimiento, lo que permite realizar tareas de ciencia de datos de manera eficiente y entrenar LLM.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a> \n",
    "# 1. Importar Librerias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "Dado que vamos hacer uso de **Google Colab** es necesario importar librerías especifícas para llevar a cabo el Fine-Tunning.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eRHe6qDY5jqS",
    "outputId": "504c9254-3c6d-47c0-e725-07480c3827a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.43.1)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.2)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.10.0)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.30.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.2.1+cu121)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.25.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install bitsandbytes transformers peft accelerate\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a> \n",
    "## 2. Preparación de Entorno Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "Dado que el entorno sobre el que estamos trabajando es **GoogleColab** es necesario tener acceso en Hugging Face. Para ello deberemos:\n",
    "\n",
    "* **Crear una cuenta en Hugging Face**\n",
    "* **Crear un Token en Hugging Face con permisos de **Escritura****\n",
    "* **Crear sobre GoogleColab la variable HF_TOKEN con el token creado**\n",
    "\n",
    "En algunas ocasiones no es suficiente crear el **HF_TOKEN**, para prevenir el error se indica un comando para poder logearse directamente con el Token creado.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VNfjwjPSFeYO",
    "outputId": "f5000b4f-35f7-4a23-d325-465d409f11fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.11.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.2.2)\n",
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
      "    Setting a new token will erase the existing one.\n",
      "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
      "Token: \n",
      "Add token as git credential? (Y/n) n\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "#Para acceder al Token\n",
    "#Creamos el Token con Write Role\n",
    "!pip install huggingface_hub\n",
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a> \n",
    "## 3. Carga del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "Realizada las configuraciones necesarias para acceder a Hugging Face, ya podemos realizar la carga de los diferentes modelos. Para la carga al igual que explicamos en el anterior notebook vamos a cargalo con un quantificación de 4 por simplicidad de memoria y comlejidad dado las limitaciones de **Google Colab**.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a> \n",
    "### 3.1 Carga del modelo ajustado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "Cargamos el modelo ajustado por los datos [recetas-cocina](https://huggingface.co/datasets/somosnlp/recetas-cocina?_sm_vck=ZbFKqrNsMLPHD4S5QDj4VD7sVRsvTLFksfWTZZq6Tktqr4fbVjrP) llamado [mistralAI_recetascocina](https://huggingface.co/guillergalre/mistralAI_recetascocina).\n",
    "\n",
    "El procedimiento es muy similar al notebook de Fine-Tunning dado que lo tenemos que cargar con quantificación 4 y deberemos cargar los Tokens y el propio modelo.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "c6f9b0732e2e46978b123c7356f84149",
      "bbb29b194b4b4253aae351033742fb66",
      "254f2e1ca8c44bff919c831f6fe56c03",
      "94cd3e2ff7464c9b812b4aa754b5fa1b",
      "86294a48f67c4b4eaebac4b1694e8322",
      "0de60e67905a4ebfa596de05ac0048b5",
      "0c9b6bcfcdf541a7818423831424f81e",
      "e602ec1bddea40c2b2ebf1b876abb187",
      "b20e2a00bcd944ef9463ca02d019c31c",
      "e5e4d6b5c3834115bc4af5e48d85f521",
      "8b2046007b6742dfbda2ce03ce4cd103"
     ]
    },
    "id": "2FJx-2YI9Q-R",
    "outputId": "475ff20f-dcad-4884-e9d9-c0e00a7d2f5f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6f9b0732e2e46978b123c7356f84149",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "#Modelo Preentrenado\n",
    "new_model = \"guillergalre/mistralAI_recetascocina\"\n",
    "tokenizerrece = AutoTokenizer.from_pretrained(new_model)\n",
    "\n",
    "tokenizerrece.pad_token = tokenizerrece.unk_token\n",
    "tokenizerrece.padding_side = \"right\"\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "use_4bit = True\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "device_map = \"auto\"\n",
    "modelrecetas = AutoModelForCausalLM.from_pretrained(\n",
    "    new_model,\n",
    "    quantization_config=bnb_config,  # loading model in 4-bit\n",
    "    device_map=device_map, # to use max gpu resources if exist\n",
    ")\n",
    "\n",
    "#Configure the pad token in the model\n",
    "modelrecetas.config.pad_token_id = tokenizerrece.pad_token_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a> \n",
    "### 3.2 Carga del modelo base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "Cargamos el modelo base [Mistral-7B-Instruct-v0.1](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1).\n",
    "\n",
    "El procedimiento es muy similar al notebook de Fine-Tunning dado que lo tenemos que cargar con quantificación 4 y deberemos cargar los Tokens y el propio modelo.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "d3b20f86cb4e43f0b479a66df13907bf",
      "cbdee992c0cb43e189dbffbbc3a845c8",
      "4c114493190d4a07be2b3d39f8c4554b",
      "b98860ba0b5d4ffb87b5aed38b0d7882",
      "c24b2ebbbe7f4f1eb20b638de83c6129",
      "b6cf649b23314aafb8c23d8641719960",
      "90b6038ac8164d0f95613439378e4c17",
      "8153c656316a47d1ab34c87243591028",
      "9919d3200b3147a8b26e7fe8e7ab1dd2",
      "e4e06b5550be40178b0505db6b674129",
      "ee0766375464485f93d6cb05bc6f4810"
     ]
    },
    "id": "fq-OmiRPBjhu",
    "outputId": "d5605376-05bd-4532-d123-f378ac2e4d1f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3b20f86cb4e43f0b479a66df13907bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Modelo MIstral\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "mistral = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "tokenizermistral = AutoTokenizer.from_pretrained(mistral)\n",
    "\n",
    "tokenizermistral.pad_token = tokenizermistral.unk_token\n",
    "tokenizermistral.padding_side = \"right\"\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "use_4bit = True\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "device_map = \"auto\"\n",
    "modelmistral = AutoModelForCausalLM.from_pretrained(\n",
    "    mistral,\n",
    "    quantization_config=bnb_config,  # loading model in 4-bit\n",
    "    device_map=device_map # to use max gpu resources if exist\n",
    ")\n",
    "\n",
    "#Configure the pad token in the model\n",
    "modelmistral.config.pad_token_id = tokenizermistral.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a> \n",
    "## 4. Inferencia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "La inferencia consisitirá en poner en práctica lo que el modelo ha aprendido durante el ajuste.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"7\"></a> \n",
    "### 4.1 Preprocesado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "Dado que tenemos que introducir los datos de entrada de acuerdo a la plantilla con la que se ha ajustado/entrenado tenemos que realizar un función que transforme dichas peticiones.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "XZf690eoOnMp"
   },
   "outputs": [],
   "source": [
    "def build_prompt(question):\n",
    "    prompt = '<s>[INST]'+ question +'[/INST]</s>'\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"8\"></a> \n",
    "### 4.2 Inferencia modelo ajustado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "A partir del modelo cargado realizaremos un pipeline indicandole que la máxima longitud es de 300 palabras. Por otro lado llamaremos a la función creada previamente para preparar la petición de acuerdo a la planilla.\n",
    "\n",
    "Uno de los parámetros importates en la configuración del pipeline será la task, en nuestro caso tendremos que indicar que genere un texto a partir de la petición indicadada. Para ello el parámetro será **text-generation**.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sp6iaMUExfg-",
    "outputId": "69c861e2-dc0c-4d96-96ed-8c0bc6766aa0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respondeme a la pregunta haciendo una lista: ¿Como harias un Sudado de Pollo? 1. Poner en una olla grande el agua, el pollo, la cebolla, el ajo, el pimentón, el orégano, el comino, la sal y la pimienta. 2. Llevar a ebullición y dejar cocer durante 1 hora. 3. Retirar del fuego y dejar enfriar. 4. Desmenuzar el pollo y mezclar con la salsa. 5. Servir en una fuente y espolvorear con perejil.\n",
      "--- 20.701589584350586 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "pipe = pipeline(task=\"text-generation\", model=modelrecetas, tokenizer=tokenizerrece, max_length=3000)\n",
    "\n",
    "question = \"Respondeme a la pregunta haciendo una lista: ¿Como harias un Sudado de Pollo?\"\n",
    "prompt = build_prompt(question)\n",
    "\n",
    "result = pipe(question)\n",
    "\n",
    "print(result[0]['generated_text'])\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1RfKkcqCCT14"
   },
   "source": [
    "<a id=\"9\"></a> \n",
    "### 4.3 Inferencia modelo base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "A partir del modelo base realizaremos un pipeline indicandole que la máxima longitud es de 300 palabras. Por otro lado llamaremos a la función creada previamente para preparar la petición de acuerdo a la planilla.\n",
    "\n",
    "Uno de los parámetros importates en la configuración del pipeline será la task, en nuestro caso tendremos que indicar que genere un texto a partir de la petición indicadada. Para ello el parámetro será **text-generation**.\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rgrpfOXABQ7l",
    "outputId": "4e9ea377-2347-49f6-d604-5795dfc054dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respondeme a la pregunta haciendo una lista: ¿Como harias un Sudado de Pollo?\n",
      "\n",
      "1. En una olla grande, agregue 2 cucharadas de aceite de oliva y caliente sobre fuego mediano.\n",
      "2. Agregue 1 cucharada de pimienta, 1 cucharada de sal y 1 cucharada de pimienta negra.\n",
      "3. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "4. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "5. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "6. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "7. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "8. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "9. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "10. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "11. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "12. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "13. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "14. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "15. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "16. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "17. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "18. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "19. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "20. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "21. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "22. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "23. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "24. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "25. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "26. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "27. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "28. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "29. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "30. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "31. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "32. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "33. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "34. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "35. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "36. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "37. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "38. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "39. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "40. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "41. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "42. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "43. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "44. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "45. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "46. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "47. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "48. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "49. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "50. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "51. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "52. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "53. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "54. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "55. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "56. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "57. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "58. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "59. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "60. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "61. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "62. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "63. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "64. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "65. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "66. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "67. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "68. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "69. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "70. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "71. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "72. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "73. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "74. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "75. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "76. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "77. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "78. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "79. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "80. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "81. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "82. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "83. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "84. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "85. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "86. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "87. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "88. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "89. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "90. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "91. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "92. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "93. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "94. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "95. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "96. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "97. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "98. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "99. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "100. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "101. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "102. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "103. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "104. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "105. Agregue 1 cucharada de pimienta negra y 1 cucharada de sal.\n",
      "106. Agregue 1 cucharada de pimienta neg\n",
      "--- 203.39856100082397 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "pipe = pipeline(task=\"text-generation\", model=modelmistral, tokenizer=tokenizermistral, max_length=3000)\n",
    "\n",
    "question = \"Respondeme a la pregunta haciendo una lista: ¿Como harias un Sudado de Pollo?\"\n",
    "prompt = build_prompt(question)\n",
    "\n",
    "result = pipe(question)\n",
    "\n",
    "print(result[0]['generated_text'])\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"10\"></a> \n",
    "## 5. Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pesar que la muestra sobre la que ajustamos el modelo es pequeña (~5K) podemos comprobar que el modelo base no es capaz de devolver una solucción razonable en coparación con el modelo ajustado.\n",
    "\n",
    "Podemos decir que el experimento es **SATISFACTORIO**."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0c9b6bcfcdf541a7818423831424f81e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0de60e67905a4ebfa596de05ac0048b5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "254f2e1ca8c44bff919c831f6fe56c03": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e602ec1bddea40c2b2ebf1b876abb187",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b20e2a00bcd944ef9463ca02d019c31c",
      "value": 2
     }
    },
    "4c114493190d4a07be2b3d39f8c4554b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8153c656316a47d1ab34c87243591028",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9919d3200b3147a8b26e7fe8e7ab1dd2",
      "value": 2
     }
    },
    "8153c656316a47d1ab34c87243591028": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "86294a48f67c4b4eaebac4b1694e8322": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8b2046007b6742dfbda2ce03ce4cd103": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "90b6038ac8164d0f95613439378e4c17": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "94cd3e2ff7464c9b812b4aa754b5fa1b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e5e4d6b5c3834115bc4af5e48d85f521",
      "placeholder": "​",
      "style": "IPY_MODEL_8b2046007b6742dfbda2ce03ce4cd103",
      "value": " 2/2 [01:15&lt;00:00, 35.15s/it]"
     }
    },
    "9919d3200b3147a8b26e7fe8e7ab1dd2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b20e2a00bcd944ef9463ca02d019c31c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b6cf649b23314aafb8c23d8641719960": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b98860ba0b5d4ffb87b5aed38b0d7882": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e4e06b5550be40178b0505db6b674129",
      "placeholder": "​",
      "style": "IPY_MODEL_ee0766375464485f93d6cb05bc6f4810",
      "value": " 2/2 [01:14&lt;00:00, 34.60s/it]"
     }
    },
    "bbb29b194b4b4253aae351033742fb66": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0de60e67905a4ebfa596de05ac0048b5",
      "placeholder": "​",
      "style": "IPY_MODEL_0c9b6bcfcdf541a7818423831424f81e",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "c24b2ebbbe7f4f1eb20b638de83c6129": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c6f9b0732e2e46978b123c7356f84149": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bbb29b194b4b4253aae351033742fb66",
       "IPY_MODEL_254f2e1ca8c44bff919c831f6fe56c03",
       "IPY_MODEL_94cd3e2ff7464c9b812b4aa754b5fa1b"
      ],
      "layout": "IPY_MODEL_86294a48f67c4b4eaebac4b1694e8322"
     }
    },
    "cbdee992c0cb43e189dbffbbc3a845c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b6cf649b23314aafb8c23d8641719960",
      "placeholder": "​",
      "style": "IPY_MODEL_90b6038ac8164d0f95613439378e4c17",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "d3b20f86cb4e43f0b479a66df13907bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cbdee992c0cb43e189dbffbbc3a845c8",
       "IPY_MODEL_4c114493190d4a07be2b3d39f8c4554b",
       "IPY_MODEL_b98860ba0b5d4ffb87b5aed38b0d7882"
      ],
      "layout": "IPY_MODEL_c24b2ebbbe7f4f1eb20b638de83c6129"
     }
    },
    "e4e06b5550be40178b0505db6b674129": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e5e4d6b5c3834115bc4af5e48d85f521": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e602ec1bddea40c2b2ebf1b876abb187": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ee0766375464485f93d6cb05bc6f4810": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
